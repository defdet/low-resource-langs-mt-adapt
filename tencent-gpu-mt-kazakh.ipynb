{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81391a9a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T16:59:00.472295Z",
     "iopub.status.busy": "2026-01-02T16:59:00.472027Z",
     "iopub.status.idle": "2026-01-02T16:59:31.088440Z",
     "shell.execute_reply": "2026-01-02T16:59:31.087887Z"
    },
    "papermill": {
     "duration": 30.621271,
     "end_time": "2026-01-02T16:59:31.090055",
     "exception": false,
     "start_time": "2026-01-02T16:59:00.468784",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "from time import strftime\n",
    "from typing import Optional, Dict, Union, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import datasets\n",
    "from datasets import Dataset, load_dataset, concatenate_datasets\n",
    "import evaluate\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    DataCollatorWithPadding,\n",
    "    GPTNeoXConfig,\n",
    "    LlamaConfig,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    T5Config,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    logging as hf_logging,\n",
    ")\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "chrf_metric = evaluate.load(\"chrf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f74ec31",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T16:59:31.096948Z",
     "iopub.status.busy": "2026-01-02T16:59:31.095702Z",
     "iopub.status.idle": "2026-01-02T16:59:31.100102Z",
     "shell.execute_reply": "2026-01-02T16:59:31.099428Z"
    },
    "papermill": {
     "duration": 0.008972,
     "end_time": "2026-01-02T16:59:31.101477",
     "exception": false,
     "start_time": "2026-01-02T16:59:31.092505",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ['CLEARML_API_ACCESS_KEY'] = ''\n",
    "os.environ['CLEARML_API_SECRET_KEY'] = ''\n",
    "os.environ['CLEARML_API_HOST'] = 'https://api.clear.ml'\n",
    "os.environ['CLEARML_LOG_MODEL'] = 'FALSE'\n",
    "os.environ['CLEARML_TASK'] = 'ru-kazakh-post'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b375dcd5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T16:59:31.392321Z",
     "iopub.status.busy": "2026-01-02T16:59:31.392070Z",
     "iopub.status.idle": "2026-01-02T17:01:29.155083Z",
     "shell.execute_reply": "2026-01-02T17:01:29.154118Z"
    },
    "papermill": {
     "duration": 117.767895,
     "end_time": "2026-01-02T17:01:29.156579",
     "exception": false,
     "start_time": "2026-01-02T16:59:31.388684",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EN-CHV train: 353306 examples\n",
      "EN-CHV test: 18596 examples\n",
      "\n",
      "Train: 353306 examples (EN+RU -> CHV, shuffled)\n",
      "Test: 10 examples (EN -> CHV)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import datasets\n",
    "from datasets import concatenate_datasets\n",
    "\n",
    "MODEL = 'tencent/HY-MT1.5-7B'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "def build_text_en_tt(example):\n",
    "    \"\"\"\n",
    "    Format English->Chuvash training examples using HY-MT1.5 chat template.\n",
    "    \"\"\"\n",
    "    source_text = example['en']\n",
    "    target_text = example['tt']\n",
    "    \n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Translate the following segment into kazakh, without additional explanation.\\n\\n{source_text}\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": target_text\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False\n",
    "    )\n",
    "    \n",
    "    example[\"text\"] = text\n",
    "    return example\n",
    "\n",
    "def build_text_en_tt(example):\n",
    "    \"\"\"\n",
    "    Format Russian->Chuvash training examples using HY-MT1.5 chat template.\n",
    "    \"\"\"\n",
    "    source_text = example['ru']\n",
    "    target_text = example['kk']\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Translate the following segment into Kazakh, without additional explanation.\\n\\n{source_text}\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": target_text\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False\n",
    "    )\n",
    "    \n",
    "    example[\"text\"] = text\n",
    "    return example\n",
    "\n",
    "def tokenize_example(example):\n",
    "    \"\"\"\n",
    "    Tokenize the formatted text for training.\n",
    "    \"\"\"\n",
    "    encoded = tokenizer(\n",
    "        example[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=False\n",
    "    )\n",
    "    example[\"input_ids\"] = encoded[\"input_ids\"]\n",
    "    example[\"attention_mask\"] = encoded[\"attention_mask\"]\n",
    "    \n",
    "    example[\"labels\"] = encoded[\"input_ids\"].copy()\n",
    "    \n",
    "    return example\n",
    "\n",
    "ru_kz_full = datasets.load_dataset('issai/kazparc')['train']\n",
    "\n",
    "\n",
    "ru_kz_split = ru_kz_full.train_test_split(test_size=0.05, seed=42)\n",
    "ru_kz_train = ru_kz_split['train']\n",
    "ru_kz_test = ru_kz_split['test']\n",
    "\n",
    "ru_kz_train = ru_kz_train.map(build_text_en_tt, num_proc=64)\n",
    "ru_kz_train = ru_kz_train.map(tokenize_example, batched=False, num_proc=64)\n",
    "\n",
    "ru_kz_test = ru_kz_test.map(build_text_en_tt, num_proc=8)\n",
    "ru_kz_test = ru_kz_test.map(tokenize_example, batched=False, num_proc=16)\n",
    "\n",
    "print(f\"EN-CHV train: {len(ru_kz_train)} examples\")\n",
    "print(f\"EN-CHV test: {len(ru_kz_test)} examples\")\n",
    "\n",
    "\n",
    "ds = datasets.DatasetDict({\n",
    "    'train': ru_kz_train,\n",
    "    'test': ru_kz_test\n",
    "})\n",
    "\n",
    "ds['test'] = ds['test'].select(range(10))\n",
    "\n",
    "print(f\"\\nTrain: {len(ds['train'])} examples (EN+RU -> CHV, shuffled)\")\n",
    "print(f\"Test: {len(ds['test'])} examples (EN -> CHV)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "01309c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|startoftext|>Translate the following segment into Kazakh, without additional explanation.\n",
      "\n",
      "Для контрактов на переработку техногенных минеральных образований стартовый размер подписного бонуса устанавливается по формуле (С1 х 0,01%), но не менее 300-кратного размера месячного расчётного показателя, установленного законом о республиканском бюджете и действующего на дату опубликования условий конкурса или дату подписания протокола прямых переговоров по предоставлению права недропользования в соответствии с законодательством Республики Казахстан о недрах и недропользовании.<|extra_0|>Қол қою бонусының бастапқы мөлшері техногендік минералдық түзілімдерді қайта өңдеуге арналған келісімшарттар үшін (Қ1 х 0,01%) формуласы бойынша белгіленеді, бірақ республикалық бюджет туралы заңда белгіленген және конкурс шарттары жарияланған күнге немесе Қазақстан Республикасының жер қойнауы және жер қойнауын пайдалану туралы заңнамасына сәйкес жер қойнауын пайдалану құқығын беру жөніндегі тікелей келіссөздер хаттамасына қол қойылған күнге қолданыста болатын айлық есептік көрсеткіштің 300 еселенген мөлшерінен кем емес болуы керек.<|eos|>\n"
     ]
    }
   ],
   "source": [
    "print(ds['test']['text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a466e04",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T17:01:29.166122Z",
     "iopub.status.busy": "2026-01-02T17:01:29.165517Z",
     "iopub.status.idle": "2026-01-02T17:01:29.177278Z",
     "shell.execute_reply": "2026-01-02T17:01:29.176512Z"
    },
    "papermill": {
     "duration": 0.018035,
     "end_time": "2026-01-02T17:01:29.178793",
     "exception": false,
     "start_time": "2026-01-02T17:01:29.160758",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds['train'] = ds['train'].remove_columns(['kk', 'ru', 'text'])\n",
    "ds['test'] = ds['test'].remove_columns(['kk', 'ru', 'text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d45de2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T17:01:29.187618Z",
     "iopub.status.busy": "2026-01-02T17:01:29.187175Z",
     "iopub.status.idle": "2026-01-02T17:02:01.358440Z",
     "shell.execute_reply": "2026-01-02T17:02:01.357443Z"
    },
    "papermill": {
     "duration": 32.177923,
     "end_time": "2026-01-02T17:02:01.360502",
     "exception": false,
     "start_time": "2026-01-02T17:01:29.182579",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50880a0d7dec4d80ac650986954007cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MODEL = 'tencent/HY-MT1.5-7B'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL, padding_side='left')\n",
    "\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL, \n",
    "    dtype=torch.bfloat16, \n",
    ").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fc2a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 13,631,488 || all params: 7,518,199,808 || trainable%: 0.1813\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "target_modules = [\n",
    "    \"q_proj\",\n",
    "    \"k_proj\",\n",
    "    \"v_proj\",\n",
    "    \"o_proj\",\n",
    "]\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=target_modules,\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d847c7a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T17:02:01.392862Z",
     "iopub.status.busy": "2026-01-02T17:02:01.392549Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": false,
     "start_time": "2026-01-02T17:02:01.387108",
     "status": "running"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 127958}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClearML Task: created new task id=d1d4188cd1db44a289d7646105d0fd40\n",
      "2026-01-12 04:30:23,070 - clearml.Task - INFO - No repository found, storing script code instead\n",
      "ClearML results page: https://app.clear.ml/projects/c26a6bbc77a549af84da889d0ac9231f/experiments/d1d4188cd1db44a289d7646105d0fd40/output/log\n",
      "2026-01-12 04:30:26,360 - clearml.Task - WARNING - Parameters must be of builtin type (Transformers/accelerator_config[AcceleratorConfig])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='257' max='44164' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  257/44164 03:24 < 9:47:00, 1.25 it/s, Epoch 0.01/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "IS_7B = True\n",
    "if IS_7B:\n",
    "    ASSISTANT_TOKEN = 127962\n",
    "else:\n",
    "    ASSISTANT_TOKEN = 120007\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "\n",
    "    # 2) If preds are logits: (bs, seq, vocab) -> take argmax\n",
    "    if isinstance(preds, np.ndarray) and preds.ndim == 3:\n",
    "        preds = np.argmax(preds, axis=-1)\n",
    "\n",
    "    # 3) Replace -100 (or any negative ids) in both preds and labels\n",
    "    preds = np.where(preds < 0, tokenizer.pad_token_id, preds)\n",
    "    labels = np.where(labels < 0, tokenizer.pad_token_id, labels)\n",
    "\n",
    "    # 4) just to be sure\n",
    "    preds = preds.astype(np.int32)\n",
    "    labels = labels.astype(np.int32)\n",
    "\n",
    "    # 5) Slice to keep only tokens after assistant token (not including it)\n",
    "    truncated_preds = []\n",
    "    truncated_labels = []\n",
    "    \n",
    "    for pred_seq, label_seq in zip(preds, labels):\n",
    "        pred_asst_pos = np.where(pred_seq == ASSISTANT_TOKEN)[0]\n",
    "        if len(pred_asst_pos) > 0:\n",
    "            start_pos = pred_asst_pos[0] + 1\n",
    "            truncated_preds.append(pred_seq[start_pos:])\n",
    "        else:\n",
    "            truncated_preds.append(pred_seq)\n",
    "        \n",
    "        # Find assistant token in labels\n",
    "        label_asst_pos = np.where(label_seq == ASSISTANT_TOKEN)[0]\n",
    "        if len(label_asst_pos) > 0:\n",
    "            start_pos = label_asst_pos[0] + 1\n",
    "            truncated_labels.append(label_seq[start_pos:])\n",
    "        else:\n",
    "            truncated_labels.append(label_seq)\n",
    "\n",
    "    # Decode only the answer portions (no prompts)\n",
    "    decoded_preds = tokenizer.batch_decode(truncated_preds, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(truncated_labels, skip_special_tokens=True)\n",
    "\n",
    "    decoded_preds = [p.strip() for p in decoded_preds]\n",
    "    decoded_labels = [l.strip() for l in decoded_labels]\n",
    "    \n",
    "    print(f'Generated (first 3): {decoded_preds[:10]}')\n",
    "    print(f'Labels (first 3): {decoded_labels[:10]}')\n",
    "    \n",
    "    result = chrf_metric.compute(\n",
    "        predictions=decoded_preds,\n",
    "        references=decoded_labels,\n",
    "        word_order=2\n",
    "    )\n",
    "    return {\"chrf\": result[\"score\"]}\n",
    "\n",
    "\n",
    "class HYMTSeq2SeqTrainer(Seq2SeqTrainer):\n",
    "\n",
    "    def _mask_prompt_labels(self, inputs):\n",
    "        if \"labels\" not in inputs or \"input_ids\" not in inputs:\n",
    "            return inputs\n",
    "\n",
    "        input_ids = inputs[\"input_ids\"]\n",
    "        labels = inputs[\"labels\"]\n",
    "        if not torch.is_tensor(labels):\n",
    "            return inputs\n",
    "\n",
    "        labels = labels.clone()\n",
    "\n",
    "        for i in range(input_ids.size(0)):\n",
    "            pos = (input_ids[i] == ASSISTANT_TOKEN).nonzero(as_tuple=True)[0]\n",
    "            if pos.numel() == 0:\n",
    "                labels[i, :] = -100\n",
    "            else:\n",
    "                cut = int(pos[0].item()) + 1\n",
    "                labels[i, :cut] = -100\n",
    "\n",
    "        inputs[\"labels\"] = labels\n",
    "        return inputs\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        inputs = self._mask_prompt_labels(inputs)\n",
    "        return super().compute_loss(model, inputs, return_outputs=return_outputs, **kwargs)\n",
    "\n",
    "    def prediction_step(self, model, inputs, prediction_loss_only, ignore_keys=None, **gen_kwargs):\n",
    "        has_labels = \"labels\" in inputs\n",
    "        inputs = self._prepare_inputs(inputs)\n",
    "\n",
    "        if has_labels:\n",
    "            inputs = self._mask_prompt_labels(inputs)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if has_labels:\n",
    "                loss, _ = self.compute_loss(model, inputs, return_outputs=True)\n",
    "                loss = loss.mean().detach()\n",
    "            else:\n",
    "                loss = None\n",
    "\n",
    "        if prediction_loss_only:\n",
    "            return (loss, None, None)\n",
    "\n",
    "        if self.args.predict_with_generate:\n",
    "            ASSISTANT_TOKEN = 120007\n",
    "            input_ids = inputs[\"input_ids\"]\n",
    "            batch_size = input_ids.shape[0]\n",
    "\n",
    "            truncated_ids = []\n",
    "            truncated_masks = []\n",
    "\n",
    "            for i in range(batch_size):\n",
    "                seq = input_ids[i]\n",
    "                pos = (seq == ASSISTANT_TOKEN).nonzero(as_tuple=True)[0]\n",
    "                if pos.numel() > 0:\n",
    "                    cut = int(pos[0].item()) + 1\n",
    "                    seq = seq[:cut]\n",
    "                truncated_ids.append(seq)\n",
    "                truncated_masks.append(torch.ones_like(seq))\n",
    "\n",
    "            max_len = max(x.numel() for x in truncated_ids)\n",
    "            pad_token_id = self.processing_class.pad_token_id\n",
    "\n",
    "            padded_ids = []\n",
    "            padded_masks = []\n",
    "            for ids, mask in zip(truncated_ids, truncated_masks):\n",
    "                pad_len = max_len - ids.numel()\n",
    "                if pad_len > 0:\n",
    "                    ids = torch.cat([ids, torch.full((pad_len,), pad_token_id, dtype=ids.dtype, device=ids.device)])\n",
    "                    mask = torch.cat([mask, torch.zeros(pad_len, dtype=mask.dtype, device=mask.device)])\n",
    "                padded_ids.append(ids)\n",
    "                padded_masks.append(mask)\n",
    "\n",
    "            gru_inputs = {\n",
    "                \"input_ids\": torch.stack(padded_ids),\n",
    "                \"attention_mask\": torch.stack(padded_masks),\n",
    "            }\n",
    "\n",
    "            gen_kwargs = self._gen_kwargs.copy()\n",
    "            gen_kwargs = dict(\n",
    "                max_new_tokens=512,\n",
    "                num_beams=1,\n",
    "                do_sample=True,\n",
    "                num_return_sequences=1,\n",
    "                repetition_penalty=1.1,\n",
    "                early_stopping=True,\n",
    "                temperature=0.7,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "            )\n",
    "            if gen_kwargs.get(\"max_length\") is None and gen_kwargs.get(\"max_new_tokens\") is None:\n",
    "                gen_kwargs[\"max_new_tokens\"] = self.args.generation_max_length\n",
    "            if gen_kwargs.get(\"num_beams\") is None:\n",
    "                gen_kwargs[\"num_beams\"] = self.args.generation_num_beams\n",
    "\n",
    "            generated_tokens = model.module.generate(**gru_inputs, **gen_kwargs)\n",
    "        else:\n",
    "            generated_tokens = None\n",
    "\n",
    "        labels = inputs[\"labels\"] if has_labels else None\n",
    "        return (loss, generated_tokens, labels)\n",
    "\n",
    "\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    padding=True,\n",
    "    pad_to_multiple_of=8,\n",
    "    label_pad_token_id=-100\n",
    ")\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"checkpoints_7B_lora_translated/ru-kz-final\",\n",
    "\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=1,\n",
    "\n",
    "    # Learning rate\n",
    "    learning_rate=2e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.05,\n",
    "\n",
    "    # Optimizer\n",
    "    optim=\"adamw_torch\",\n",
    "    weight_decay=0.01,\n",
    "\n",
    "    # Gradient handling\n",
    "    max_grad_norm=1.0,\n",
    "\n",
    "    # Logging\n",
    "    logging_steps=10,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "\n",
    "    # Evaluation\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=1_000,\n",
    "\n",
    "    # Generation for chrf++\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=512,  # Max tokens to generate\n",
    "    generation_num_beams=5,  # Beam search for better quality\n",
    "\n",
    "    # Saving\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=1_000,\n",
    "    save_total_limit=1,\n",
    "    save_only_model=True,\n",
    "    save_safetensors=False,\n",
    "    load_best_model_at_end=False,\n",
    "    metric_for_best_model=\"chrf\",  # Use chrF++ for best model\n",
    "    greater_is_better=True,  # Higher chrF is better\n",
    "\n",
    "    dataloader_num_workers=4,\n",
    "    dataloader_pin_memory=True,\n",
    "\n",
    "    # Reproducibility\n",
    "    seed=42,\n",
    "\n",
    "    # Reporting\n",
    "    report_to=\"clearml\",\n",
    "\n",
    "    remove_unused_columns=True,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "\n",
    "trainer = HYMTSeq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=ds['train'],\n",
    "    eval_dataset=ds['test'],\n",
    "    data_collator=data_collator,\n",
    "    processing_class=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ed87f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('checkpoints_7B_lora_translated/tatar_final_lora/tokenizer_config.json',\n",
       " 'checkpoints_7B_lora_translated/tatar_final_lora/special_tokens_map.json',\n",
       " 'checkpoints_7B_lora_translated/tatar_final_lora/chat_template.jinja',\n",
       " 'checkpoints_7B_lora_translated/tatar_final_lora/tokenizer.json')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained('checkpoints_7B_lora_translated/kazakh_final_lora')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1dc0b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 83b06db4-5513-4c6d-baa0-8e5a3b0ece78)')' thrown while requesting HEAD https://huggingface.co/tencent/HY-MT1.5-7B/resolve/main/config.json\n",
      "Retrying in 1s [Retry 1/5].\n"
     ]
    }
   ],
   "source": [
    "trainer.model.save_pretrained('checkpoints_7B_lora_translated/kazakh_final_lora')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31236,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "bolgov_simson_training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-01-02T16:58:37.796629",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
